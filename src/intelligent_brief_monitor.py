"""
Intelligent Campaign Brief Monitoring System
Real-time, AI-powered brief detection and analysis with multi-source integration
"""

import asyncio
import json
import os
import hashlib
import yaml
from datetime import datetime
from typing import Dict, List, Any, Optional
from pathlib import Path
from dataclasses import dataclass
from enum import Enum
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import imaplib
import requests
import logging

class BriefSource(Enum):
    FILESYSTEM = "filesystem"
    EMAIL = "email"
    SLACK = "slack"
    API = "api"
    CLOUD_STORAGE = "cloud_storage"
    WEBHOOK = "webhook"

@dataclass
class BriefMetadata:
    """Comprehensive brief metadata with AI analysis"""
    source: BriefSource
    detected_at: datetime
    file_hash: str
    size_bytes: int
    language: str
    complexity_score: float
    urgency_score: float
    quality_score: float
    completeness_percentage: float
    estimated_variants: int
    estimated_cost: float
    risk_factors: List[str]
    required_approvals: List[str]
    dependencies: List[str]
    ai_recommendations: List[str]

class IntelligentBriefMonitor:
    """Advanced brief monitoring with real-time detection and AI analysis"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.brief_cache = {}
        self.watchdog_observer = None
        self.external_monitors = {}
        self.ai_analyzer = AIBriefAnalyzer()
        
        # Real-time monitoring configuration
        self.monitor_config = {
            "filesystem_paths": ["campaign_briefs", "input", "briefs"],
            "email_accounts": self._load_email_config(),
            "slack_channels": self._load_slack_config(),
            "api_endpoints": self._load_api_config(),
            "cloud_storage": self._load_cloud_config(),
            "webhook_endpoints": self._setup_webhook_listeners()
        }
        
        # Advanced detection settings
        self.detection_settings = {
            "file_types": [".yaml", ".yml", ".json", ".docx", ".pdf"],
            "min_file_size": 100,  # bytes
            "max_file_size": 10 * 1024 * 1024,  # 10MB
            "duplicate_detection": True,
            "version_tracking": True,
            "auto_validation": True,
            "ai_content_analysis": True
        }
    
    async def start_comprehensive_monitoring(self):
        """Start all monitoring sources simultaneously"""
        self.logger.info("üîç Starting comprehensive brief monitoring...")
        
        # Start filesystem monitoring with real-time events
        await self._start_filesystem_monitoring()
        
        # Start external source monitoring
        monitoring_tasks = [
            self._monitor_email_sources(),
            self._monitor_slack_channels(),
            self._monitor_api_endpoints(),
            self._monitor_cloud_storage(),
            self._monitor_webhook_queue()
        ]
        
        # Run all monitoring tasks concurrently
        await asyncio.gather(*monitoring_tasks, return_exceptions=True)
    
    async def _start_filesystem_monitoring(self):
        """Real-time filesystem monitoring with watchdog"""
        class BriefFileHandler(FileSystemEventHandler):
            def __init__(self, monitor):
                self.monitor = monitor
            
            def on_created(self, event):
                if not event.is_directory:
                    asyncio.create_task(self.monitor._process_new_file(event.src_path, "created"))
            
            def on_modified(self, event):
                if not event.is_directory:
                    asyncio.create_task(self.monitor._process_new_file(event.src_path, "modified"))
            
            def on_moved(self, event):
                if not event.is_directory:
                    asyncio.create_task(self.monitor._process_new_file(event.dest_path, "moved"))
        
        self.watchdog_observer = Observer()
        event_handler = BriefFileHandler(self)
        
        # Monitor multiple directories
        for path in self.monitor_config["filesystem_paths"]:
            if os.path.exists(path):
                self.watchdog_observer.schedule(event_handler, path, recursive=True)
                self.logger.info(f"üìÅ Monitoring filesystem path: {path}")
        
        self.watchdog_observer.start()
    
    async def _process_new_file(self, file_path: str, event_type: str):
        """Process newly detected files with comprehensive analysis"""
        try:
            file_path_obj = Path(file_path)
            
            # Quick validation
            if not self._is_valid_brief_file(file_path_obj):
                return
            
            self.logger.info(f"üìÑ Processing {event_type} file: {file_path}")
            
            # Check for duplicates
            file_hash = self._calculate_file_hash(file_path_obj)
            if file_hash in self.brief_cache:
                self.logger.info(f"‚ö†Ô∏è Duplicate file detected: {file_path}")
                return
            
            # Load and analyze content
            brief_content = await self._load_brief_content(file_path_obj)
            if not brief_content:
                return
            
            # AI-powered analysis
            metadata = await self.ai_analyzer.analyze_brief(brief_content, file_path_obj)
            metadata.source = BriefSource.FILESYSTEM
            metadata.detected_at = datetime.now()
            metadata.file_hash = file_hash
            
            # Cache and notify
            self.brief_cache[file_hash] = {
                "file_path": str(file_path_obj),
                "content": brief_content,
                "metadata": metadata,
                "processed_at": datetime.now()
            }
            
            # Trigger processing pipeline
            await self._trigger_brief_processing(file_hash, brief_content, metadata)
            
        except Exception as e:
            self.logger.error(f"‚ùå Error processing file {file_path}: {e}")
    
    async def _monitor_email_sources(self):
        """Monitor email accounts for campaign briefs"""
        while True:
            try:
                for email_config in self.monitor_config["email_accounts"]:
                    await self._check_email_account(email_config)
                await asyncio.sleep(60)  # Check emails every minute
            except Exception as e:
                self.logger.error(f"‚ùå Email monitoring error: {e}")
                await asyncio.sleep(300)  # Wait 5 minutes on error
    
    async def _check_email_account(self, config: Dict[str, str]):
        """Check specific email account for new briefs"""
        try:
            # Connect to email server
            mail = imaplib.IMAP4_SSL(config["server"])
            mail.login(config["username"], config["password"])
            mail.select("INBOX")
            
            # Search for campaign brief emails
            search_criteria = 'SUBJECT "campaign brief" OR SUBJECT "creative brief"'
            _, message_ids = mail.search(None, search_criteria)
            
            for msg_id in message_ids[0].split():
                await self._process_email_brief(mail, msg_id, config)
            
            mail.logout()
            
        except Exception as e:
            self.logger.error(f"‚ùå Email check error for {config['username']}: {e}")
    
    async def _monitor_slack_channels(self):
        """Monitor Slack channels for brief uploads and mentions"""
        slack_config = self.monitor_config.get("slack", {})
        if not slack_config.get("token"):
            self.logger.info("‚ö†Ô∏è Slack monitoring disabled - no token configured")
            return

        headers = {"Authorization": f"Bearer {slack_config['token']}"}
        channels = slack_config.get("channels", [])
        last_timestamps = {}

        while True:
            try:
                for channel in channels:
                    # Get channel history
                    params = {"channel": channel, "limit": 10}
                    if channel in last_timestamps:
                        params["oldest"] = last_timestamps[channel]

                    response = requests.get(
                        "https://slack.com/api/conversations.history",
                        headers=headers,
                        params=params,
                        timeout=30
                    )

                    if response.status_code == 200:
                        data = response.json()
                        if data.get("ok"):
                            for message in data.get("messages", []):
                                # Check for file attachments (briefs)
                                files = message.get("files", [])
                                for file_info in files:
                                    if file_info.get("name", "").endswith(('.yaml', '.yml', '.json')):
                                        await self._process_slack_file(file_info, headers)
                                # Update timestamp
                                last_timestamps[channel] = message.get("ts", last_timestamps.get(channel))

                await asyncio.sleep(30)  # Check Slack every 30 seconds
            except Exception as e:
                self.logger.error(f"‚ùå Slack monitoring error: {e}")
                await asyncio.sleep(60)

    async def _process_slack_file(self, file_info: Dict, headers: Dict):
        """Download and process a file from Slack"""
        try:
            file_url = file_info.get("url_private_download")
            if file_url:
                response = requests.get(file_url, headers=headers, timeout=30)
                if response.status_code == 200:
                    # Save to temp location and process
                    temp_path = Path(f"cache/slack_{file_info['id']}_{file_info['name']}")
                    temp_path.parent.mkdir(exist_ok=True)
                    temp_path.write_bytes(response.content)
                    await self._process_new_file(str(temp_path), "slack_upload")
                    self.logger.info(f"üì• Downloaded Slack file: {file_info['name']}")
        except Exception as e:
            self.logger.error(f"‚ùå Error processing Slack file: {e}")

    async def _monitor_api_endpoints(self):
        """Monitor REST API endpoints for brief submissions"""
        api_config = self.monitor_config.get("api_endpoints", [])
        if not api_config:
            self.logger.info("‚ö†Ô∏è API monitoring disabled - no endpoints configured")
            return

        while True:
            try:
                for endpoint in api_config:
                    url = endpoint.get("url")
                    method = endpoint.get("method", "GET")
                    auth_header = endpoint.get("auth_header", {})

                    response = requests.request(
                        method,
                        url,
                        headers=auth_header,
                        timeout=30
                    )

                    if response.status_code == 200:
                        briefs = response.json()
                        if isinstance(briefs, list):
                            for brief_data in briefs:
                                brief_id = brief_data.get("id", hashlib.md5(str(brief_data).encode()).hexdigest()[:8])
                                if brief_id not in self.brief_cache:
                                    # Save brief to file and process
                                    temp_path = Path(f"cache/api_{brief_id}.yaml")
                                    temp_path.parent.mkdir(exist_ok=True)
                                    with open(temp_path, 'w') as f:
                                        yaml.dump(brief_data, f)
                                    await self._process_new_file(str(temp_path), "api_fetch")
                                    self.logger.info(f"üì° Fetched brief from API: {brief_id}")

                await asyncio.sleep(endpoint.get("poll_interval", 60))
            except Exception as e:
                self.logger.error(f"‚ùå API monitoring error: {e}")
                await asyncio.sleep(120)

    async def _monitor_cloud_storage(self):
        """Monitor cloud storage (S3, Google Drive, etc.) for new briefs"""
        cloud_config = self.monitor_config.get("cloud_storage", {})
        provider = cloud_config.get("provider")

        if not provider:
            self.logger.info("‚ö†Ô∏è Cloud storage monitoring disabled - no provider configured")
            return

        while True:
            try:
                if provider == "s3":
                    await self._check_s3_bucket(cloud_config)
                elif provider == "gcs":
                    await self._check_gcs_bucket(cloud_config)
                elif provider == "azure":
                    await self._check_azure_container(cloud_config)

                await asyncio.sleep(cloud_config.get("poll_interval", 120))
            except Exception as e:
                self.logger.error(f"‚ùå Cloud storage monitoring error: {e}")
                await asyncio.sleep(300)

    async def _check_s3_bucket(self, config: Dict):
        """Check AWS S3 bucket for new briefs"""
        try:
            import boto3
            s3 = boto3.client('s3',
                aws_access_key_id=config.get("access_key"),
                aws_secret_access_key=config.get("secret_key"),
                region_name=config.get("region", "us-east-1")
            )
            bucket = config.get("bucket")
            prefix = config.get("prefix", "briefs/")

            response = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)
            for obj in response.get("Contents", []):
                key = obj["Key"]
                if key.endswith(('.yaml', '.yml', '.json')):
                    etag = obj["ETag"]
                    if etag not in self.brief_cache:
                        # Download and process
                        temp_path = Path(f"cache/s3_{Path(key).name}")
                        temp_path.parent.mkdir(exist_ok=True)
                        s3.download_file(bucket, key, str(temp_path))
                        await self._process_new_file(str(temp_path), "s3_download")
                        self.logger.info(f"‚òÅÔ∏è Downloaded from S3: {key}")
        except ImportError:
            self.logger.warning("‚ö†Ô∏è boto3 not installed - S3 monitoring unavailable")
        except Exception as e:
            self.logger.error(f"‚ùå S3 check error: {e}")

    async def _check_gcs_bucket(self, config: Dict):
        """Check Google Cloud Storage bucket for new briefs"""
        try:
            from google.cloud import storage
            client = storage.Client()
            bucket = client.bucket(config.get("bucket"))
            prefix = config.get("prefix", "briefs/")

            blobs = bucket.list_blobs(prefix=prefix)
            for blob in blobs:
                if blob.name.endswith(('.yaml', '.yml', '.json')):
                    if blob.md5_hash not in self.brief_cache:
                        temp_path = Path(f"cache/gcs_{Path(blob.name).name}")
                        temp_path.parent.mkdir(exist_ok=True)
                        blob.download_to_filename(str(temp_path))
                        await self._process_new_file(str(temp_path), "gcs_download")
                        self.logger.info(f"‚òÅÔ∏è Downloaded from GCS: {blob.name}")
        except ImportError:
            self.logger.warning("‚ö†Ô∏è google-cloud-storage not installed - GCS monitoring unavailable")
        except Exception as e:
            self.logger.error(f"‚ùå GCS check error: {e}")

    async def _check_azure_container(self, config: Dict):
        """Check Azure Blob Storage container for new briefs"""
        try:
            from azure.storage.blob import BlobServiceClient
            connection_string = config.get("connection_string")
            container_name = config.get("container")
            prefix = config.get("prefix", "briefs/")

            blob_service = BlobServiceClient.from_connection_string(connection_string)
            container = blob_service.get_container_client(container_name)

            for blob in container.list_blobs(name_starts_with=prefix):
                if blob.name.endswith(('.yaml', '.yml', '.json')):
                    if blob.etag not in self.brief_cache:
                        temp_path = Path(f"cache/azure_{Path(blob.name).name}")
                        temp_path.parent.mkdir(exist_ok=True)
                        blob_client = container.get_blob_client(blob.name)
                        with open(temp_path, 'wb') as f:
                            f.write(blob_client.download_blob().readall())
                        await self._process_new_file(str(temp_path), "azure_download")
                        self.logger.info(f"‚òÅÔ∏è Downloaded from Azure: {blob.name}")
        except ImportError:
            self.logger.warning("‚ö†Ô∏è azure-storage-blob not installed - Azure monitoring unavailable")
        except Exception as e:
            self.logger.error(f"‚ùå Azure check error: {e}")

    async def _monitor_webhook_queue(self):
        """Process incoming webhook notifications"""
        webhook_config = self.monitor_config.get("webhook", {})
        queue_path = Path(webhook_config.get("queue_path", "cache/webhook_queue"))

        if not webhook_config.get("enabled", False):
            self.logger.info("‚ö†Ô∏è Webhook monitoring disabled")
            return

        queue_path.mkdir(parents=True, exist_ok=True)

        while True:
            try:
                # Process any queued webhook payloads
                for payload_file in queue_path.glob("*.json"):
                    try:
                        with open(payload_file, 'r') as f:
                            payload = json.load(f)

                        # Extract brief data from webhook payload
                        brief_data = payload.get("brief") or payload.get("data") or payload
                        brief_id = payload.get("id", payload_file.stem)

                        if brief_id not in self.brief_cache:
                            # Save as YAML and process
                            temp_path = Path(f"cache/webhook_{brief_id}.yaml")
                            with open(temp_path, 'w') as f:
                                yaml.dump(brief_data, f)
                            await self._process_new_file(str(temp_path), "webhook")
                            self.logger.info(f"üîî Processed webhook brief: {brief_id}")

                        # Remove processed payload
                        payload_file.unlink()

                    except Exception as e:
                        self.logger.error(f"‚ùå Error processing webhook payload {payload_file}: {e}")

                await asyncio.sleep(5)  # Check webhook queue every 5 seconds
            except Exception as e:
                self.logger.error(f"‚ùå Webhook monitoring error: {e}")
                await asyncio.sleep(30)
    
    def _is_valid_brief_file(self, file_path: Path) -> bool:
        """Validate if file could be a campaign brief"""
        # Check file extension
        if file_path.suffix.lower() not in self.detection_settings["file_types"]:
            return False
        
        # Check file size
        try:
            size = file_path.stat().st_size
            if size < self.detection_settings["min_file_size"] or size > self.detection_settings["max_file_size"]:
                return False
        except (OSError, IOError):
            return False
        
        return True
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """Calculate SHA-256 hash of file for duplicate detection"""
        hasher = hashlib.sha256()
        try:
            with open(file_path, 'rb') as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hasher.update(chunk)
            return hasher.hexdigest()
        except (OSError, IOError):
            return f"error_{file_path.name}_{datetime.now().timestamp()}"
    
    async def _load_brief_content(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """Load and parse brief content from various formats"""
        try:
            if file_path.suffix.lower() in ['.yaml', '.yml']:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return yaml.safe_load(f)
            elif file_path.suffix.lower() == '.json':
                with open(file_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            elif file_path.suffix.lower() in ['.docx', '.pdf']:
                # Placeholder for document parsing
                return {"text_content": f"Document: {file_path.name}", "needs_manual_review": True}
            else:
                return None
        except Exception as e:
            self.logger.error(f"‚ùå Error loading brief content from {file_path}: {e}")
            return None
    
    async def _trigger_brief_processing(self, file_hash: str, content: Dict[str, Any], metadata: BriefMetadata):
        """Trigger the campaign processing pipeline"""
        self.logger.info(f"üöÄ Triggering processing for brief {file_hash}")
        
        # Create processing event
        processing_event = {
            "event_type": "brief_detected",
            "file_hash": file_hash,
            "content": content,
            "metadata": metadata.__dict__,
            "timestamp": datetime.now().isoformat()
        }
        
        # Trigger campaign processing via pipeline integration
        self.logger.info(f"üìã Brief processing triggered: {processing_event}")

        # Try to trigger via pipeline integration
        try:
            from .pipeline_integration import PipelineIntegration
            pipeline = PipelineIntegration()

            # Create campaign ID from brief
            campaign_id = content.get("campaign_id") or f"brief_{hashlib.md5(json.dumps(content, sort_keys=True).encode()).hexdigest()[:8]}"

            # Trigger generation asynchronously
            asyncio.create_task(
                pipeline.trigger_generation(campaign_id, content)
            )
            self.logger.info(f"üöÄ Pipeline triggered for campaign: {campaign_id}")

        except ImportError:
            self.logger.warning("Pipeline integration not available - brief logged but not processed")
        except Exception as e:
            self.logger.error(f"Failed to trigger pipeline: {e}")
    
    def _load_email_config(self) -> List[Dict[str, str]]:
        """Load email monitoring configuration"""
        return [
            {
                "server": "imap.gmail.com",
                "username": os.getenv("EMAIL_USERNAME", ""),
                "password": os.getenv("EMAIL_PASSWORD", ""),
                "folder": "INBOX"
            }
        ]
    
    def _load_slack_config(self) -> List[str]:
        """Load Slack channel monitoring configuration"""
        return ["#creative-briefs", "#campaign-requests", "#urgent-briefs"]
    
    def _load_api_config(self) -> List[Dict[str, str]]:
        """Load API endpoint monitoring configuration"""
        return [
            {"url": "https://api.company.com/briefs", "method": "GET", "auth_token": os.getenv("API_TOKEN", "")}
        ]
    
    def _load_cloud_config(self) -> Dict[str, str]:
        """Load cloud storage monitoring configuration"""
        return {
            "aws_s3_bucket": "campaign-briefs-bucket",
            "google_drive_folder": "Creative Automation/Briefs",
            "dropbox_folder": "/Campaign Briefs"
        }
    
    def _setup_webhook_listeners(self) -> List[str]:
        """Setup webhook listener endpoints"""
        return ["/webhooks/briefs", "/webhooks/slack", "/webhooks/email"]


class AIBriefAnalyzer:
    """AI-powered brief analysis and quality assessment"""
    
    def __init__(self):
        self.openai_client = None
        if os.getenv("OPENAI_API_KEY"):
            try:
                from openai import OpenAI
                self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            except Exception as e:
                logging.getLogger(__name__).warning(f"OpenAI initialization failed: {e}")
    
    async def analyze_brief(self, content: Dict[str, Any], file_path: Path) -> BriefMetadata:
        """Comprehensive AI analysis of campaign brief"""
        
        # Basic metadata
        metadata = BriefMetadata(
            source=BriefSource.FILESYSTEM,
            detected_at=datetime.now(),
            file_hash="",
            size_bytes=file_path.stat().st_size if file_path.exists() else 0,
            language="en",
            complexity_score=0.0,
            urgency_score=0.0,
            quality_score=0.0,
            completeness_percentage=0.0,
            estimated_variants=0,
            estimated_cost=0.0,
            risk_factors=[],
            required_approvals=[],
            dependencies=[],
            ai_recommendations=[]
        )
        
        # Analyze content structure and completeness
        metadata.completeness_percentage = self._calculate_completeness(content)
        metadata.complexity_score = self._calculate_complexity(content)
        metadata.urgency_score = self._calculate_urgency(content)
        
        # AI-powered analysis if available
        if self.openai_client:
            ai_analysis = await self._ai_analyze_content(content)
            metadata.quality_score = ai_analysis.get("quality_score", 0.5)
            metadata.ai_recommendations = ai_analysis.get("recommendations", [])
            metadata.risk_factors = ai_analysis.get("risk_factors", [])
        
        # Estimate variants and cost
        metadata.estimated_variants = self._estimate_variants(content)
        metadata.estimated_cost = self._estimate_cost(content, metadata.estimated_variants)
        
        return metadata
    
    def _calculate_completeness(self, content: Dict[str, Any]) -> float:
        """Calculate brief completeness percentage"""
        required_fields = [
            "campaign_name", "client", "products", "target_audience",
            "deliverables", "timeline", "budget", "brand_guidelines"
        ]
        
        present_fields = sum(1 for field in required_fields if field in content and content[field])
        return (present_fields / len(required_fields)) * 100
    
    def _calculate_complexity(self, content: Dict[str, Any]) -> float:
        """Calculate campaign complexity score"""
        complexity_factors = {
            "products": len(content.get("products", [])) * 0.1,
            "deliverables": len(str(content.get("deliverables", {}))) * 0.01,
            "custom_requirements": len(content.get("custom_requirements", [])) * 0.2,
            "localization": len(content.get("localization", {}).get("languages", [])) * 0.15,
            "approval_workflow": len(content.get("approval_workflow", [])) * 0.1
        }
        
        return min(sum(complexity_factors.values()), 1.0)
    
    def _calculate_urgency(self, content: Dict[str, Any]) -> float:
        """Calculate urgency score based on timeline and tags"""
        urgency_score = 0.0
        
        # Check timeline
        timeline = content.get("timeline", {})
        if "deadline" in timeline:
            try:
                deadline = datetime.fromisoformat(timeline["deadline"])
                days_until = (deadline - datetime.now()).days
                if days_until <= 1:
                    urgency_score += 0.8
                elif days_until <= 3:
                    urgency_score += 0.6
                elif days_until <= 7:
                    urgency_score += 0.4
            except (ValueError, TypeError):
                # Invalid deadline format, skip deadline-based urgency calculation
                pass
        
        # Check tags
        tags = content.get("tags", [])
        urgent_tags = ["urgent", "rush", "asap", "emergency", "critical"]
        if any(tag.lower() in urgent_tags for tag in tags):
            urgency_score += 0.5
        
        return min(urgency_score, 1.0)
    
    async def _ai_analyze_content(self, content: Dict[str, Any]) -> Dict[str, Any]:
        """Use AI to analyze brief content quality and provide recommendations"""
        try:
            prompt = f"""
            Analyze this campaign brief and provide:
            1. Quality score (0-1)
            2. Potential risk factors
            3. Recommendations for improvement
            
            Brief content: {json.dumps(content, indent=2)}
            
            Respond in JSON format with: quality_score, risk_factors (array), recommendations (array)
            """
            
            response = self.openai_client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=500,
                temperature=0.3
            )
            
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            logging.getLogger(__name__).warning(f"AI analysis failed: {e}")
            return {
                "quality_score": 0.5,
                "risk_factors": ["Unable to perform AI analysis"],
                "recommendations": ["Review brief manually for completeness"]
            }
    
    def _estimate_variants(self, content: Dict[str, Any]) -> int:
        """Estimate number of variants to be generated"""
        products = len(content.get("products", []))
        deliverables = content.get("deliverables", {})
        
        aspect_ratios = len(deliverables.get("aspect_ratios", ["1:1", "16:9", "9:16"]))
        variants_per_product = deliverables.get("variants_per_product", 3)
        languages = len(content.get("localization", {}).get("languages", ["en"]))
        
        return products * aspect_ratios * variants_per_product * languages
    
    def _estimate_cost(self, content: Dict[str, Any], estimated_variants: int) -> float:
        """Estimate campaign cost based on complexity and variants"""
        base_cost_per_variant = 25.0  # Base cost
        
        # Complexity multiplier
        complexity_multiplier = 1.0 + (self._calculate_complexity(content) * 0.5)
        
        # Priority multiplier
        priority_tags = content.get("tags", [])
        priority_multiplier = 1.5 if any(tag in ["urgent", "premium"] for tag in priority_tags) else 1.0
        
        return estimated_variants * base_cost_per_variant * complexity_multiplier * priority_multiplier


# Example usage and demo
async def demo_intelligent_monitoring():
    """Demonstrate the intelligent brief monitoring system"""
    monitor = IntelligentBriefMonitor()
    
    print("üîç Starting Intelligent Brief Monitoring Demo...")
    print("üìã Features:")
    print("  ‚úÖ Real-time filesystem monitoring with watchdog")
    print("  ‚úÖ Multi-source integration (email, Slack, API, cloud)")
    print("  ‚úÖ AI-powered content analysis and quality scoring")
    print("  ‚úÖ Duplicate detection and version tracking")
    print("  ‚úÖ Comprehensive metadata extraction")
    print("  ‚úÖ Risk assessment and recommendation engine")
    
    # Start monitoring (in demo mode, we'll just show the setup)
    print("\nüöÄ Monitoring system initialized and ready!")
    
    return monitor


if __name__ == "__main__":
    asyncio.run(demo_intelligent_monitoring())